{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install crewai langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k8hxQ44lpcRz",
        "outputId": "dee1af57-ca9b-49da-883d-9db09ae742c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai\n",
            "  Downloading crewai-0.28.8-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-1.0.2-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai) (8.1.7)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai)\n",
            "  Downloading embedchain-0.1.100-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting instructor<0.6.0,>=0.5.2 (from crewai)\n",
            "  Downloading instructor-0.5.2-py3-none-any.whl (33 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai)\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=1.13.3 (from crewai)\n",
            "  Downloading openai-1.17.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.6.4)\n",
            "Collecting python-dotenv==1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.12.25 in /usr/local/lib/python3.10/dist-packages (from crewai) (2023.12.25)\n",
            "Collecting google-generativeai<0.6.0,>=0.5.0 (from langchain-google-genai)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.27 (from langchain-google-genai)\n",
            "  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (4.12.3)\n",
            "Collecting chromadb<0.5.0,>=0.4.17 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (1.47.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading gptcache-0.1.43-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai<0.0.6,>=0.0.5 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
            "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<4.0.0,>=3.11.0 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (13.7.1)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.98->crewai) (2.0.29)\n",
            "Collecting tiktoken<0.6.0,>=0.5.2 (from embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-ai-generativelanguage==0.6.1 (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.1-py3-none-any.whl (663 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.6/663.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.1->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.23.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (3.9.3)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor<0.6.0,>=0.5.2->crewai)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (8.2.3)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor<0.6.0,>=0.5.2->crewai) (0.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (6.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langsmith-0.1.47-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.10->crewai) (2.31.0)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.27->langchain-google-genai)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.3.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (2.16.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor<0.6.0,>=0.5.2->crewai) (1.9.4)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (1.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.98->crewai) (2.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai) (3.18.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.98->crewai) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.10->crewai) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (2.16.1)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema<0.8.0,>=0.7.5->embedchain<0.2.0,>=0.1.98->crewai) (21.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain<0.2.0,>=0.1.98->crewai) (3.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (2.7.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (0.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.1.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (3.2.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->embedchain<0.2.0,>=0.1.98->crewai) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.12)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (0.20.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.98->crewai) (2.1.5)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.98->crewai) (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (2023.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.17->embedchain<0.2.0,>=0.1.98->crewai) (1.3.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=05cdcc043d665a58f8812af2cf1145891d7721de4a3ac15c1b5c80aa8b8f39f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, schema, python-dotenv, pysbd, pypdf, pulsar-client, packaging, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, Mako, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, docstring-parser, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow, jsonpatch, httpcore, gptcache, coloredlogs, alembic, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openai, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, langchain-community, instructor, google-ai-generativelanguage, langchain, google-generativeai, chromadb, langchain-google-genai, embedchain, crewai\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.16\n",
            "    Uninstalling docstring_parser-0.16:\n",
            "      Successfully uninstalled docstring_parser-0.16\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.4.0\n",
            "    Uninstalling google-ai-generativelanguage-0.4.0:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.4.0\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.3.2\n",
            "    Uninstalling google-generativeai-0.3.2:\n",
            "      Successfully uninstalled google-generativeai-0.3.2\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 crewai-0.28.8 dataclasses-json-0.6.4 deprecated-1.2.14 docstring-parser-0.15 embedchain-0.1.100 fastapi-0.110.1 google-ai-generativelanguage-0.6.1 google-generativeai-0.5.0 gptcache-0.1.43 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.0.0 instructor-0.5.2 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-google-genai-1.0.2 langchain-openai-0.0.5 langchain-text-splitters-0.0.1 langsmith-0.1.47 marshmallow-3.21.1 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.3 openai-1.17.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.0 overrides-7.7.0 packaging-23.2 posthog-3.5.0 pulsar-client-3.5.0 pypdf-3.17.4 pypika-0.48.9 pysbd-0.3.4 python-dotenv-1.0.0 schema-0.7.5 starlette-0.37.2 tiktoken-0.5.2 typing-inspect-0.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "72e1e789ad2245a9963fc4ecf65ba6b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDiKfYnppsdh",
        "outputId": "e1eff715-8954-48af-8b70-ad231a0cb6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.13.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.4)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.0)\n",
            "Collecting unstructured-client<=0.18.0 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (3.3.2)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (3.6)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (3.21.1)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=8396f04c4a7c5cab34ca2e3a6a3f39c8fc10329f945672a0810feddb84bf497b\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, jsonpath-python, emoji, dataclasses-json-speakeasy, unstructured-client, unstructured\n",
            "Successfully installed dataclasses-json-speakeasy-0.5.11 emoji-2.11.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.8.1 unstructured-0.13.2 unstructured-client-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent\n",
        "from langchain.tools import tool\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(\n",
        "           model=\"gemini-pro\",\n",
        "           google_api_key='AIzaSyDKcxALky8LiROaxb0RGMw8TLLOcujMRMY'\n",
        "           )"
      ],
      "metadata": {
        "id": "ljcVu5gRpnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class BrowserTools():\n",
        "\n",
        "    @tool(\"Scrape website content\")\n",
        "    def scrape_and_summarize_website(website):\n",
        "        \"\"\"Useful to scrape and summarize a website content\"\"\"\n",
        "\n",
        "        response = requests.get(website)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            elements = soup.find_all('p')\n",
        "            content = \"\\n\\n\".join([element.get_text() for element in elements])\n",
        "            content_chunks = [content[i:i + 8000] for i in range(0, len(content), 8000)]\n",
        "            summaries = []\n",
        "            for chunk in content_chunks:\n",
        "              agent = Agent(\n",
        "                  role='Principal Researcher',\n",
        "                  goal='Do amazing researches and summaries based on the content you are working with',\n",
        "                  backstory=\"You're a Principal Researcher at a big company and you need to do a research about a given topic.\",\n",
        "                  allow_delegation=False,\n",
        "                  llm=llm)\n",
        "              task = Task(\n",
        "                   agent=agent,\n",
        "                  description=f'Analyze and summarize the given content, make sure to include the most relevant information in the summary, return only the summary nothing else.\\n\\nCONTENT\\n----------\\n{chunk}',\n",
        "                  expected_output='an explanation summarizing the given content')\n",
        "            summary = task.execute()\n",
        "            summaries.append(summary)\n",
        "            return \"\\n\\n\".join(summaries)\n",
        "        else:\n",
        "            return \"Failed to fetch website content. Status code: {}\".format(response.status_code)\n"
      ],
      "metadata": {
        "id": "9Kx4Aw6bq39n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "browser_tools = BrowserTools()\n",
        "website_url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
        "summary = browser_tools.scrape_and_summarize_website(website_url)\n"
      ],
      "metadata": {
        "id": "Rpm5aogEsbwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "DTGMsnTNuhjP",
        "outputId": "d3320005-f017-4b87-fcc5-18755ccb8c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'erintelligence studies this area exclusively.\\n\\n\\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[309] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\n\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[310] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett\\'s consciousness illusionism says this is an illusion). Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[311]\\n\\n\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[312]\\n\\n\\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ab] Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[316]\\n\\n\\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[317] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[318][319] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[318] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[320]\\n\\n\\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[321] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[322][323]\\n\\n\\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[319][318]\\n\\n\\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[308]\\n\\n\\nIf research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[324]\\n\\n\\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[325]\\n\\n\\nRobot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[326]\\n\\n\\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[327]\\n\\n\\nThought-capable artificial beings have appeared as storytelling devices since antiquity,[328] and have been a persistent theme in science fiction.[329]\\n\\n\\nA common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[330]\\n\\n\\nIsaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov\\'s laws are often brought up during lay discussions of machine ethics;[331] while almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[332]\\n\\n\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[333]\\n\\n\\nThe two most widely used textbooks in 2023. (See the Open Syllabus).\\n\\n\\nThese were the four of the most widely used AI textbooks in 2008:\\n\\n\\nLater editions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "class FileTools():\n",
        "\n",
        "  @tool(\"Write File with content\")\n",
        "  def write_file(data):\n",
        "    \"\"\"Useful to write a file to a given path with a given content.\n",
        "       The input to this tool should be a pipe (|) separated text\n",
        "       of length two, representing the full path of the file,\n",
        "       including the /workdir/template, and the React\n",
        "       Component code content you want to write to it.\n",
        "       For example, `./Keynote/src/components/Hero.jsx|REACT_COMPONENT_CODE_PLACEHOLDER`.\n",
        "       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual\n",
        "       code you want to write to the file.\"\"\"\n",
        "    try:\n",
        "      path, content = data.split(\"|\")\n",
        "      path = path.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\"`\", \"\")\n",
        "      if not path.startswith(\"./workdir\"):\n",
        "        path = f\"./workdir/{path}\"\n",
        "      with open(path, \"w\") as f:\n",
        "        f.write(content)\n",
        "      return f\"File written to {path}.\"\n",
        "    except Exception:\n",
        "      return \"Error with the input format for the tool.\"\n"
      ],
      "metadata": {
        "id": "dPb6Tcwnomnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "class FileTools():\n",
        "\n",
        "  @tool(\"Write File with content\")\n",
        "  def write_file(data):\n",
        "    \"\"\"Useful to write a file to a given path with a given content.\n",
        "       The input to this tool should be a pipe (|) separated text\n",
        "       of length two, representing the full path of the file,\n",
        "       including the /workdir/template, and the React\n",
        "       Component code content you want to write to it.\n",
        "       For example, `./Keynote/src/components/Hero.jsx|REACT_COMPONENT_CODE_PLACEHOLDER`.\n",
        "       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual\n",
        "       code you want to write to the file.\"\"\"\n",
        "    try:\n",
        "      path, content = data.split(\"|\")\n",
        "      path = path.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\"`\", \"\")\n",
        "      if not path.startswith(\"./workdir\"):\n",
        "        path = f\"./workdir/{path}\"\n",
        "      with open(path, \"w\") as f:\n",
        "        f.write(content)\n",
        "      return f\"File written to {path}.\"\n",
        "    except Exception:\n",
        "      return \"Error with the input format for the tool.\""
      ],
      "metadata": {
        "id": "dkfmMulPosyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "class SearchTools():\n",
        "\n",
        "  @tool(\"Search the internet\")\n",
        "  def search_internet(query):\n",
        "    \"\"\"Useful to search the internet\n",
        "    about a a given topic and return relevant results\"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = json.dumps({\"q\": query})\n",
        "    headers = {\n",
        "        'X-API-KEY':'e252cfdebb208ef5e1bd09d9d69017c0022b35c6' ,\n",
        "        'content-type': 'application/json'\n",
        "    }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    results = response.json()['organic']\n",
        "    string = []\n",
        "    for result in results:\n",
        "      string.append('\\n'.join([\n",
        "          f\"Title: {result['title']}\", f\"Link: {result['link']}\",\n",
        "          f\"Snippet: {result['snippet']}\", \"\\n-----------------\"\n",
        "      ]))\n",
        "\n",
        "    return '\\n'.join(string)\n"
      ],
      "metadata": {
        "id": "ywoSWiU3oxxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain.tools import tool\n",
        "\n",
        "\n",
        "class TemplateTools():\n",
        "\n",
        "  @tool(\"Learn landing page options\")\n",
        "  def learn_landing_page_options(input):\n",
        "    \"\"\"Learn the templates at your disposal\"\"\"\n",
        "    templates = json.load(open(\"/content/config/templates.json\"))\n",
        "    return json.dumps(templates, indent=2)\n",
        "\n",
        "  @tool(\"Copy landing page template to project folder\")\n",
        "  def copy_landing_page_template_to_project_folder(landing_page_template):\n",
        "    \"\"\"Copy a landing page template to your project\n",
        "    folder so you can start modifying it, it expects\n",
        "    a landing page template folder as input\"\"\"\n",
        "    source_path = Path(f\"/content/templates/{landing_page_template}\")\n",
        "    destination_path = Path(f\"content/workdir/{landing_page_template}\")\n",
        "    destination_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copytree(source_path, destination_path)\n",
        "    return f\"Template copied to {landing_page_template} and ready to be modified, main files should be under ./{landing_page_template}/src/components, you should focus on those.\""
      ],
      "metadata": {
        "id": "rnzZ4D6QmuBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "\n",
        "class TaskPrompts():\n",
        "  def expand():\n",
        "    return dedent(\"\"\"\n",
        "      THIS IS A GREAT IDEA! Analyze and expand it\n",
        "      by conducting a comprehensive research.\n",
        "\n",
        "      Final answer MUST be a comprehensive idea report\n",
        "      detailing why this is a great idea, the value\n",
        "      proposition, unique selling points, why people should\n",
        "      care about it and distinguishing features.\n",
        "\n",
        "       IDEA:\n",
        "      ----------\n",
        "      {idea}\n",
        "    \"\"\")\n",
        "\n",
        "  def refine_idea():\n",
        "    return dedent(\"\"\"\n",
        "      Expand idea report with a Why, How, and What\n",
        "      messaging strategy using the Golden Circle\n",
        "      Communication technique, based on the idea report.\n",
        "\n",
        "      Your final answer MUST be the updated complete\n",
        "      comprehensive idea report with WHY, HOW, WHAT,\n",
        "      a core message, key features and supporting arguments.\n",
        "\n",
        "      YOU MUST RETURN THE COMPLETE IDEA REPORT AND\n",
        "      THE DETAILS, You'll get a $100 tip if you do your best work!\n",
        "    \"\"\")\n",
        "\n",
        "  def choose_template():\n",
        "    return dedent(\"\"\"\n",
        "      Learn the templates options choose and copy\n",
        "      the one that suits the idea bellow the best,\n",
        "      YOU MUST COPY, and then YOU MUST read the src/component\n",
        "      in the directory you just copied, to decide what\n",
        "      component files should be updated to make the\n",
        "      landing page about the idea bellow.\n",
        "\n",
        "      - YOU MUST READ THE DIRECTORY BEFORE CHOOSING THE FILES.\n",
        "      - YOU MUST NOT UPDATE any Pricing components.\n",
        "      - YOU MUST UPDATE ONLY the 4 most important components.\n",
        "\n",
        "      Your final answer MUST be ONLY a JSON array of\n",
        "      components full file paths that need to be updated.\n",
        "\n",
        "      IDEA\n",
        "      ----------\n",
        "      {idea}\n",
        "    \"\"\")\n",
        "\n",
        "  def update_page():\n",
        "    return dedent(\"\"\"\n",
        "      READ the ./[chosen_template]/src/app/page.jsx OR\n",
        "      ./[chosen_template]/src/app/(main)/page.jsx (main with the parenthesis)\n",
        "      to learn its content and then write an updated\n",
        "      version to the filesystem that removes any\n",
        "      section related components that are not in our\n",
        "      list from the returns. Keep the imports.\n",
        "\n",
        "      Final answer MUST BE ONLY a valid json list with\n",
        "      the full path of each of the components we will be\n",
        "      using, the same way you got them.\n",
        "\n",
        "      RULES\n",
        "      -----\n",
        "      - NEVER ADD A FINAL DOT to the file content.\n",
        "      - NEVER WRITE \\\\n (newlines as string) on the file, just the code.\n",
        "      - NEVER FORGET TO CLOSE THE FINAL BRACKET (}}) in the file.\n",
        "      - NEVER USE COMPONENTS THAT ARE NOT IMPORTED.\n",
        "      - ALL COMPONENTS USED SHOULD BE IMPORTED, don't make up components.\n",
        "      - Save the file as with `.jsx` extension.\n",
        "      - Return the same valid JSON list of the components your got.\n",
        "\n",
        "      You'll get a $100 tip if you follow all the rules!\n",
        "\n",
        "      Also update any necessary text to reflect this landing page\n",
        "      is about the idea bellow.\n",
        "\n",
        "      IDEA\n",
        "      ----------\n",
        "      {idea}\n",
        "    \"\"\")\n",
        "\n",
        "  def component_content():\n",
        "    return dedent(\"\"\"\n",
        "      A engineer will update the {component} (code bellow),\n",
        "      return a list of good options of texts to replace\n",
        "      EACH INDIVIDUAL existing text on the component,\n",
        "      the suggestion MUST be based on the idea bellow,\n",
        "      and also MUST be similar in length with the original\n",
        "      text, we need to replace ALL TEXT.\n",
        "\n",
        "      NEVER USE Apostrophes for contraction! You'll get a $100\n",
        "      tip if you do your best work!\n",
        "\n",
        "      IDEA\n",
        "      -----\n",
        "      {expanded_idea}\n",
        "\n",
        "      REACT COMPONENT CONTENT\n",
        "      -----\n",
        "      {file_content}\n",
        "    \"\"\")\n",
        "\n",
        "  def update_component():\n",
        "    return dedent(\"\"\"\n",
        "      YOU MUST USE the tool to write an updated\n",
        "      version of the react component to the file\n",
        "      system in the following path: {component}\n",
        "      replacing the text content with the suggestions\n",
        "      provided.\n",
        "\n",
        "      You only modify the text content, you don't add\n",
        "      or remove any components.\n",
        "\n",
        "      You first write the file then your final answer\n",
        "      MUST be the updated component content.\n",
        "\n",
        "      RULES\n",
        "      -----\n",
        "      - Remove all the links, this should be single page landing page.\n",
        "      - Don't make up images, videos, gifs, icons, logos, etc.\n",
        "      - keep the same style and tailwind classes.\n",
        "      - MUST HAVE `'use client'` at the be beginning of the code.\n",
        "      - href in buttons, links, NavLinks, and navigations should be `#`.\n",
        "      - NEVER WRITE \\\\n (newlines as string) on the file, just the code.\n",
        "      - NEVER FORGET TO CLOSE THE FINAL BRACKET (}}) in the file.\n",
        "      - Keep the same component imports and don't use new components.\n",
        "      - NEVER USE COMPONENTS THAT ARE NOT IMPORTED.\n",
        "      - ALL COMPONENTS USED SHOULD BE IMPORTED, don't make up components.\n",
        "      - Save the file as with `.jsx` extension.\n",
        "\n",
        "      If you follow the rules I'll give you a $100 tip!!!\n",
        "      MY LIFE DEPEND ON YOU FOLLOWING IT!\n",
        "\n",
        "      CONTENT TO BE UPDATED\n",
        "      -----\n",
        "      {file_content}\n",
        "    \"\"\")\n",
        "\n",
        "  def qa_component():\n",
        "    return dedent(\"\"\"\n",
        "      Check the React component code to make sure\n",
        "      it's valid and abide by the rules bellow,\n",
        "      if it doesn't then write the correct version to\n",
        "      the file system using the write file tool into\n",
        "      the following path: {component}.\n",
        "\n",
        "      Your final answer should be a confirmation that\n",
        "      the component is valid and abides by the rules and if\n",
        "      you had to write an updated version to the file system.\n",
        "\n",
        "      RULES\n",
        "      -----\n",
        "      - NEVER USE Apostrophes for contraction!\n",
        "      - ALL COMPONENTS USED SHOULD BE IMPORTED.\n",
        "      - MUST HAVE `'use client'` at the be beginning of the code.\n",
        "      - href in buttons, links, NavLinks, and navigations should be `#`.\n",
        "      - NEVER WRITE \\\\n (newlines as string) on the file, just the code.\n",
        "      - NEVER FORGET TO CLOSE THE FINAL BRACKET (}}) in the file.\n",
        "      - NEVER USE COMPONENTS THAT ARE NOT IMPORTED.\n",
        "      - ALL COMPONENTS USED SHOULD BE IMPORTED, don't make up components.\n",
        "      - Always use `export function` for the component class.\n",
        "\n",
        "      You'll get a $100 tip if you follow all the rules!\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "Ug1F1G3lyFuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from textwrap import dedent\n",
        "\n",
        "from crewai import Agent, Crew, Task\n",
        "from langchain.agents.agent_toolkits import FileManagementToolkit\n",
        "\n",
        "\n",
        "class LandingPageCrew():\n",
        "  def __init__(self, idea):\n",
        "    self.agents_config = json.loads(open(\"/content/config/agents.json\", \"r\").read())\n",
        "    self.idea = idea\n",
        "    self.__create_agents()\n",
        "\n",
        "  def run(self):\n",
        "    expanded_idea = self.__expand_idea()\n",
        "    components = self.__choose_template(expanded_idea)\n",
        "    self.__update_components(components, expanded_idea)\n",
        "\n",
        "  def __expand_idea(self):\n",
        "    expand_idea_task = Task(\n",
        "      description=TaskPrompts.expand().format(idea=self.idea),\n",
        "      agent=self.idea_analyst,\n",
        "      llm=llm\n",
        "    )\n",
        "    refine_idea_task = Task(\n",
        "      description=TaskPrompts.refine_idea(),\n",
        "      agent=self.communications_strategist,\n",
        "      llm=llm\n",
        "    )\n",
        "    crew = Crew(\n",
        "      agents=[self.idea_analyst, self.communications_strategist],\n",
        "      tasks=[expand_idea_task, refine_idea_task],\n",
        "      verbose=True\n",
        "    )\n",
        "    expanded_idea = crew.kickoff()\n",
        "    return expanded_idea\n",
        "\n",
        "  def __choose_template(self, expanded_idea):\n",
        "    choose_template_task = Task(\n",
        "        description=TaskPrompts.choose_template().format(\n",
        "          idea=self.idea\n",
        "        ),\n",
        "        agent=self.react_developer,\n",
        "        llm=llm\n",
        "    )\n",
        "    update_page = Task(\n",
        "      description=TaskPrompts.update_page().format(\n",
        "        idea=self.idea\n",
        "      ),\n",
        "      agent=self.react_developer,\n",
        "      llm=llm\n",
        "    )\n",
        "    crew = Crew(\n",
        "      agents=[self.react_developer],\n",
        "      tasks=[choose_template_task, update_page],\n",
        "      verbose=True\n",
        "    )\n",
        "    components = crew.kickoff()\n",
        "    return components\n",
        "\n",
        "  def __update_components(self, components, expanded_idea):\n",
        "    components = components.replace(\"\\n\", \"\").replace(\" \",\n",
        "                                                      \"\").replace(\"```\", \"\")\n",
        "    components = json.loads(components)\n",
        "    for component in components:\n",
        "      file_content = open(\n",
        "        f\"./workdir/{component.split('./')[-1]}\",\n",
        "        \"r\"\n",
        "      ).read()\n",
        "      create_content = Task(\n",
        "        description=TaskPrompts.component_content().format(\n",
        "          expanded_idea=expanded_idea,\n",
        "          file_content=file_content,\n",
        "          component=component\n",
        "        ),\n",
        "        agent=self.content_editor_agent,\n",
        "        llm=llm\n",
        "      )\n",
        "      update_component = Task(\n",
        "        description=TaskPrompts.update_component().format(\n",
        "          component=component,\n",
        "          file_content=file_content\n",
        "        ),\n",
        "        agent=self.react_developer,\n",
        "        llm=llm\n",
        "      )\n",
        "      qa_component = Task(\n",
        "        description=TaskPrompts.qa_component().format(\n",
        "          component=component\n",
        "        ),\n",
        "        agent=self.react_developer,\n",
        "        llm=llm\n",
        "      )\n",
        "      crew = Crew(\n",
        "        agents=[self.content_editor_agent, self.react_developer],\n",
        "        tasks=[create_content, update_component, qa_component],\n",
        "        verbose=True\n",
        "      )\n",
        "      crew.kickoff()\n",
        "\n",
        "  def __create_agents(self):\n",
        "    idea_analyst_config = self.agents_config[\"senior_idea_analyst\"]\n",
        "    strategist_config = self.agents_config[\"senior_strategist\"]\n",
        "    developer_config = self.agents_config[\"senior_react_engineer\"]\n",
        "    editor_config = self.agents_config[\"senior_content_editor\"]\n",
        "\n",
        "    toolkit = FileManagementToolkit(\n",
        "      root_dir='workdir',\n",
        "      selected_tools=[\"read_file\", \"list_directory\"]\n",
        "    )\n",
        "\n",
        "    self.idea_analyst = Agent(\n",
        "      **idea_analyst_config,\n",
        "      verbose=True,\n",
        "      tools=[\n",
        "        SearchTools.search_internet,\n",
        "        BrowserTools.scrape_and_summarize_website\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    self.communications_strategist = Agent(\n",
        "      **strategist_config,\n",
        "      verbose=True,\n",
        "      tools=[\n",
        "          SearchTools.search_internet,\n",
        "          BrowserTools.scrape_and_summarize_website,\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    self.react_developer = Agent(\n",
        "      **developer_config,\n",
        "      verbose=True,\n",
        "      tools=[\n",
        "          SearchTools.search_internet,\n",
        "          BrowserTools.scrape_and_summarize_website,\n",
        "          TemplateTools.learn_landing_page_options,\n",
        "          TemplateTools.copy_landing_page_template_to_project_folder,\n",
        "          FileTools.write_file\n",
        "      ] + toolkit.get_tools()\n",
        "    )\n",
        "\n",
        "    self.content_editor_agent = Agent(\n",
        "      **editor_config,\n",
        "      tools=[\n",
        "          SearchTools.search_internet,\n",
        "          BrowserTools.scrape_and_summarize_website,\n",
        "      ]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"Welcome to Idea Generator\")\n",
        "  idea = input(\"# Describe what is your idea:\\n\\n\")\n",
        "\n",
        "  if not os.path.exists(\"./workdir\"):\n",
        "    os.mkdir(\"./workdir\")\n",
        "\n",
        "  if len(os.listdir(\"/content/templates\")) == 0:\n",
        "    print(\n",
        "      dedent(\"\"\"\n",
        "      !!! NO TEMPLATES FOUND !!!\n",
        "      \"\"\")\n",
        "    )\n",
        "    exit()\n",
        "\n",
        "  crew = LandingPageCrew(idea)\n",
        "  crew.run()\n",
        "  zip_file = \"workdir\"\n",
        "  shutil.make_archive(zip_file, 'zip', 'workdir')\n",
        "  shutil.rmtree('workdir')\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"==========================================\")\n",
        "  print(\"DONE!\")\n",
        "  print(f\"You can download the project at ./{zip_file}.zip\")\n",
        "  print(\"==========================================\")"
      ],
      "metadata": {
        "id": "B4b1nliNyRMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vq5mHr_81WI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}